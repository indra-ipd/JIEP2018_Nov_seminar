{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m0.44208\u001b[0m\u001b[0m | time: 0.077s\n",
      "| Adam | epoch: 017 | loss: 0.44208 - acc: 0.8175 -- iter: 800/846\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m0.44311\u001b[0m\u001b[0m | time: 1.093s\n",
      "| Adam | epoch: 017 | loss: 0.44311 - acc: 0.8188 | val_loss: 0.42345 - val_acc: 0.7778 -- iter: 846/846\n",
      "--\n",
      "['Age', 'SibSp', 'Parch', 'Gender', 'C', 'Q', 'S', 'Singleton', 'SmallFamily', 'LargeFamily', 'Young', 'Middle', 'Middle_2', 'Old', 'Very Old', 'Cabin_A', 'Cabin_B', 'Cabin_C', 'Cabin_D', 'Cabin_E', 'Cabin_F', 'Cabin_G', 'Cabin_U', 'Cabin_T', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Title_Master', 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Officer', 'Title_Royalty']\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/marcobeyer/titanic-with-tflearn-network\n",
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "\n",
    "#data_train = pd.read_csv('../input/train.csv')\n",
    "#data_test = pd.read_csv('../input/test.csv')\n",
    "\n",
    "data_train = pd.read_csv('./input/train.csv')\n",
    "data_test = pd.read_csv('./input/test.csv')\n",
    "\n",
    "def preprocess(data, columns_to_ignore):\n",
    "    processed_data = data.copy()\n",
    "    # リストのコピー\n",
    "    # data = [1,2,3]\n",
    "    # a = data.copy()\n",
    "    # print(a)\n",
    "    # [1, 2, 3]   \n",
    "    processed_data['Family_Size']= processed_data['SibSp']+processed_data['Parch']\n",
    "    processed_data['Gender'] = processed_data['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "    processed_data['Embarked'].fillna('S', inplace=True)\n",
    "    processed_data['C'] = processed_data['Embarked'].map(lambda s: 1 if s == 'C' else 0).astype(int)\n",
    "    processed_data['Q'] = processed_data['Embarked'].map(lambda s: 1 if s == 'Q' else 0).astype(int)\n",
    "    processed_data['S'] = processed_data['Embarked'].map(lambda s: 1 if s == 'S' else 0).astype(int)\n",
    "    processed_data['Singleton'] = processed_data['Family_Size'].map(lambda s: 1 if s == 1 else 0)\n",
    "    processed_data['SmallFamily'] = processed_data['Family_Size'].map(lambda s: 1 if 2<=s<=4 else 0)\n",
    "    processed_data['LargeFamily'] = processed_data['Family_Size'].map(lambda s: 1 if 5<=s else 0)\n",
    "    processed_data['Young'] = processed_data['Age'].map(lambda s: 1 if s<=16 else 0)\n",
    "    processed_data['Middle'] = processed_data['Age'].map(lambda s: 1 if s>16 and s<=32 else 0)\n",
    "    processed_data['Middle_2'] = processed_data['Age'].map(lambda s: 1 if s>30 and s<=40 else 0)\n",
    "    processed_data['Old'] = processed_data['Age'].map(lambda s: 1 if s>40 and s<=60 else 0)\n",
    "    processed_data['Very Old'] = processed_data['Age'].map(lambda s: 1 if s> 80 else 0)\n",
    "    \n",
    "    processed_data['Title'] = processed_data['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\n",
    "    \n",
    "    # a map of more aggregated titles\n",
    "    Title_Dictionary = {\n",
    "                        \"Capt\":       \"Officer\",\n",
    "                        \"Col\":        \"Officer\",\n",
    "                        \"Major\":      \"Officer\",\n",
    "                        \"Jonkheer\":   \"Royalty\",\n",
    "                        \"Don\":        \"Royalty\",\n",
    "                        \"Sir\" :       \"Royalty\",\n",
    "                        \"Dr\":         \"Officer\",\n",
    "                        \"Rev\":        \"Officer\",\n",
    "                        \"the Countess\":\"Royalty\",\n",
    "                        \"Dona\":       \"Royalty\",\n",
    "                        \"Mme\":        \"Mrs\",\n",
    "                        \"Mlle\":       \"Miss\",\n",
    "                        \"Ms\":         \"Mrs\",\n",
    "                        \"Mr\" :        \"Mr\",\n",
    "                        \"Mrs\" :       \"Mrs\",\n",
    "                        \"Miss\" :      \"Miss\",\n",
    "                        \"Master\" :    \"Master\",\n",
    "                        \"Lady\" :      \"Royalty\"\n",
    "\n",
    "                        }\n",
    "    \n",
    "    # we map each title\n",
    "    processed_data['Title'] = processed_data.Title.map(Title_Dictionary)\n",
    "    processed_data['Fare'].fillna(processed_data['Fare'].mean(), inplace=True)\n",
    "    processed_data['Cabin'].fillna('U', inplace=True)\n",
    "    processed_data['Cabin'] = processed_data['Cabin'].map(lambda c : c[0])\n",
    "    cabin_dummies = pd.get_dummies(processed_data['Cabin'], prefix='Cabin')\n",
    "    processed_data = pd.concat([processed_data,cabin_dummies], axis=1)\n",
    "    if not 'Cabin_T' in processed_data: processed_data['Cabin_T'] = 0\n",
    "    processed_data = pd.concat([processed_data, pd.get_dummies(processed_data['Pclass'], prefix='Pclass')], axis=1)\n",
    "    processed_data = pd.concat([processed_data, pd.get_dummies(processed_data['Title'], prefix='Title')], axis=1)\n",
    "    processed_data[\"Age\"] = processed_data.groupby(['Sex','Pclass','Title'])['Age'].transform(lambda x:  x.fillna(x.median()))\n",
    "    processed_data.drop(columns_to_ignore, errors='ignore', axis=1, inplace=True)\n",
    "    print(list(processed_data.columns.values))\n",
    "\n",
    "    #processed_data.dropna(axis=0, how='any', inplace=True)\n",
    "    return processed_data\n",
    "to_ignore=['Sex','PassengerId','Pclass','Ticket','Name','Survived','Family_Size', 'Embarked', 'Title', 'Cabin', 'Fare']\n",
    "labels = data_train['Survived']\n",
    "bin_labels = np.zeros((len(labels), 2))\n",
    "bin_labels[np.arange(len(labels)),labels] = 1.\n",
    "clean_data = preprocess(data_train, to_ignore)\n",
    "\n",
    "clean_data.to_csv(\"./input/clean_data.csv\")\n",
    "\n",
    "net = tflearn.input_data(shape=[None, clean_data.shape[1]])\n",
    "net = tflearn.fully_connected(net, 30)\n",
    "net = tflearn.dropout(net, 0.75)\n",
    "net = tflearn.fully_connected(net, 30)\n",
    "net = tflearn.dropout(net, 0.75)\n",
    "net = tflearn.fully_connected(net, 16)\n",
    "net = tflearn.dropout(net, 0.75)\n",
    "net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "model = tflearn.DNN(net)\n",
    "model.fit(np.array(clean_data, dtype=np.float32), bin_labels, n_epoch=17, batch_size=100,validation_set=0.05,show_metric=True)\n",
    "output = pd.concat([data_test['PassengerId'], pd.DataFrame(model.predict_label(np.array(preprocess(data_test,to_ignore), dtype=np.float32)), columns=['Survived', 'Not_Survived'])['Survived']], axis=1)\n",
    "output.to_csv(\"./input/titanic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       [38.,  1.,  0., ...,  1.,  0.,  0.],\n",
       "       [26.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ...,\n",
       "       [18.,  1.,  2., ...,  0.,  0.,  0.],\n",
       "       [26.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [32.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22.  1.  0. ...  0.  0.  0.]\n",
      " [38.  1.  0. ...  1.  0.  0.]\n",
      " [26.  0.  0. ...  0.  0.  0.]\n",
      " ...\n",
      " [18.  1.  2. ...  0.  0.  0.]\n",
      " [26.  0.  0. ...  0.  0.  0.]\n",
      " [32.  0.  0. ...  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(clean_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
